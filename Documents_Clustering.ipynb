{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install llm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Zd71Foc4xWV",
        "outputId": "b539d0eb-b6a3-4908-8d79-3105b293988a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llm\n",
            "  Downloading llm-0.18-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from llm) (8.1.7)\n",
            "Requirement already satisfied: openai>=1.0 in /usr/local/lib/python3.10/dist-packages (from llm) (1.54.4)\n",
            "Collecting click-default-group>=1.2.3 (from llm)\n",
            "  Downloading click_default_group-1.2.4-py2.py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting sqlite-utils>=3.37 (from llm)\n",
            "  Downloading sqlite_utils-3.37-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting sqlite-migrate>=0.1a2 (from llm)\n",
            "  Downloading sqlite_migrate-0.1b0-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: pydantic>=1.10.2 in /usr/local/lib/python3.10/dist-packages (from llm) (2.9.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from llm) (6.0.2)\n",
            "Requirement already satisfied: pluggy in /usr/local/lib/python3.10/dist-packages (from llm) (1.5.0)\n",
            "Collecting python-ulid (from llm)\n",
            "  Downloading python_ulid-3.0.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from llm) (75.1.0)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (from llm) (24.1.2)\n",
            "Collecting puremagic (from llm)\n",
            "  Downloading puremagic-1.28-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.0->llm) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.0->llm) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.0->llm) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.0->llm) (0.7.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.0->llm) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai>=1.0->llm) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai>=1.0->llm) (4.12.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10.2->llm) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10.2->llm) (2.23.4)\n",
            "Collecting sqlite-fts4 (from sqlite-utils>=3.37->llm)\n",
            "  Downloading sqlite_fts4-1.0.3-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from sqlite-utils>=3.37->llm) (0.9.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from sqlite-utils>=3.37->llm) (2.8.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.0->llm) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.0->llm) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>=1.0->llm) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>=1.0->llm) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.0->llm) (0.14.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil->sqlite-utils>=3.37->llm) (1.16.0)\n",
            "Downloading llm-0.18-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading click_default_group-1.2.4-py2.py3-none-any.whl (4.1 kB)\n",
            "Downloading sqlite_migrate-0.1b0-py3-none-any.whl (10.0 kB)\n",
            "Downloading sqlite_utils-3.37-py3-none-any.whl (67 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.7/67.7 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading puremagic-1.28-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_ulid-3.0.0-py3-none-any.whl (11 kB)\n",
            "Downloading sqlite_fts4-1.0.3-py3-none-any.whl (10.0 kB)\n",
            "Installing collected packages: sqlite-fts4, puremagic, python-ulid, click-default-group, sqlite-utils, sqlite-migrate, llm\n",
            "Successfully installed click-default-group-1.2.4 llm-0.18 puremagic-1.28 python-ulid-3.0.0 sqlite-fts4-1.0.3 sqlite-migrate-0.1b0 sqlite-utils-3.37\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JcgfVbzi4sLW"
      },
      "outputs": [],
      "source": [
        "import click\n",
        "import json\n",
        "import llm\n",
        "import numpy as np\n",
        "import sklearn.cluster\n",
        "import sqlite_utils\n",
        "import textwrap\n",
        "\n",
        "DEFAULT_SUMMARY_PROMPT = \"\"\"\n",
        "Short, concise title for this cluster of related documents.\n",
        "\"\"\".strip()\n",
        "\n",
        "\n",
        "@llm.hookimpl\n",
        "def register_commands(cli):\n",
        "    @cli.command()\n",
        "    @click.argument(\"collection\")\n",
        "    @click.argument(\"n\", type=int)\n",
        "    @click.option(\n",
        "        \"--truncate\",\n",
        "        type=int,\n",
        "        default=100,\n",
        "        help=\"Truncate content to this many characters - 0 for no truncation\",\n",
        "    )\n",
        "    @click.option(\n",
        "        \"-d\",\n",
        "        \"--database\",\n",
        "        type=click.Path(\n",
        "            file_okay=True, allow_dash=False, dir_okay=False, writable=True\n",
        "        ),\n",
        "        envvar=\"LLM_EMBEDDINGS_DB\",\n",
        "        help=\"SQLite database file containing embeddings\",\n",
        "    )\n",
        "    @click.option(\n",
        "        \"--summary\", is_flag=True, help=\"Generate summary title for each cluster\"\n",
        "    )\n",
        "    @click.option(\"-m\", \"--model\", help=\"LLM model to use for the summary\")\n",
        "    @click.option(\"--prompt\", help=\"Custom prompt to use for the summary\")\n",
        "    def cluster(collection, n, truncate, database, summary, model, prompt):\n",
        "        \"\"\"\n",
        "        Generate clusters from embeddings in a collection\n",
        "\n",
        "        Example usage, to create 10 clusters:\n",
        "\n",
        "        \\b\n",
        "            llm cluster my_collection 10\n",
        "\n",
        "        Outputs a JSON array of {\"id\": \"cluster_id\", \"items\": [list of items]}\n",
        "\n",
        "        Pass --summary to generate a summary for each cluster, using the default\n",
        "        language model or the model you specify with --model.\n",
        "        \"\"\"\n",
        "        from llm.cli import get_default_model, get_key\n",
        "\n",
        "        clustering_model = sklearn.cluster.MiniBatchKMeans(n_clusters=n, n_init=\"auto\")\n",
        "        if database:\n",
        "            db = sqlite_utils.Database(database)\n",
        "        else:\n",
        "            db = sqlite_utils.Database(llm.user_dir() / \"embeddings.db\")\n",
        "        rows = [\n",
        "            (row[0], llm.decode(row[1]), row[2])\n",
        "            for row in db.execute(\n",
        "                \"\"\"\n",
        "            select id, embedding, content from embeddings\n",
        "            where collection_id = (\n",
        "                select id from collections where name = ?\n",
        "            )\n",
        "        \"\"\",\n",
        "                [collection],\n",
        "            ).fetchall()\n",
        "        ]\n",
        "        to_cluster = np.array([item[1] for item in rows])\n",
        "        clustering_model.fit(to_cluster)\n",
        "        assignments = clustering_model.labels_\n",
        "\n",
        "        def truncate_text(text):\n",
        "            if not text:\n",
        "                return None\n",
        "            if truncate > 0:\n",
        "                return text[:truncate]\n",
        "            else:\n",
        "                return text\n",
        "\n",
        "        # Each one corresponds to an ID\n",
        "        clusters = {}\n",
        "        for (id, _, content), cluster in zip(rows, assignments):\n",
        "            clusters.setdefault(str(cluster), []).append(\n",
        "                {\"id\": str(id), \"content\": truncate_text(content)}\n",
        "            )\n",
        "        # Re-arrange into a list\n",
        "        output_clusters = [{\"id\": k, \"items\": v} for k, v in clusters.items()]\n",
        "\n",
        "        # Do we need to generate summaries?\n",
        "        if summary:\n",
        "            model = llm.get_model(model or get_default_model())\n",
        "            if model.needs_key:\n",
        "                model.key = get_key(\"\", model.needs_key, model.key_env_var)\n",
        "            prompt = prompt or DEFAULT_SUMMARY_PROMPT\n",
        "            click.echo(\"[\")\n",
        "            for cluster, is_last in zip(\n",
        "                output_clusters, [False] * (len(output_clusters) - 1) + [True]\n",
        "            ):\n",
        "                click.echo(\"  {\")\n",
        "                click.echo('    \"id\": {},'.format(json.dumps(cluster[\"id\"])))\n",
        "                click.echo(\n",
        "                    '    \"items\": '\n",
        "                    + textwrap.indent(\n",
        "                        json.dumps(cluster[\"items\"], indent=2), \"    \"\n",
        "                    ).lstrip()\n",
        "                    + \",\"\n",
        "                )\n",
        "                prompt_content = \"\\n\".join(\n",
        "                    [item[\"content\"] for item in cluster[\"items\"] if item[\"content\"]]\n",
        "                )\n",
        "                if prompt_content.strip():\n",
        "                    summary = model.prompt(\n",
        "                        prompt_content,\n",
        "                        system=prompt,\n",
        "                    ).text()\n",
        "                else:\n",
        "                    summary = None\n",
        "                click.echo('    \"summary\": {}'.format(json.dumps(summary)))\n",
        "                click.echo(\"  }\" + (\",\" if not is_last else \"\"))\n",
        "            click.echo(\"]\")\n",
        "        else:\n",
        "            click.echo(json.dumps(output_clusters, indent=4))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import llm\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "\n",
        "\n",
        "# Create a collection and embed the documents\n",
        "collection = llm.Collection(\"documents\", model_id=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "embeddings = []\n",
        "valid_documents = []\n",
        "\n",
        "for i, doc in enumerate(documents):\n",
        "    try:\n",
        "        embedding = collection.embed(f\"doc_{i}\", doc, store=True)\n",
        "        if embedding is not None:\n",
        "            embeddings.append(embedding)\n",
        "            valid_documents.append(doc)\n",
        "        else:\n",
        "            print(f\"Warning: Embedding for document {i} is None\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error embedding document {i}: {str(e)}\")\n",
        "\n",
        "# Print the number of embeddings\n",
        "print(f\"Number of embeddings: {len(embeddings)}\")\n",
        "\n",
        "# Convert embeddings to numpy array\n",
        "if embeddings:\n",
        "    embeddings_array = np.array(embeddings)\n",
        "    print(f\"Shape of embeddings array: {embeddings_array.shape}\")\n",
        "\n",
        "    # Check for NaN values\n",
        "    nan_count = np.isnan(embeddings_array).sum()\n",
        "    print(f\"Number of NaN values: {nan_count}\")\n",
        "\n",
        "    # Remove any NaN values\n",
        "    embeddings_array = embeddings_array[~np.isnan(embeddings_array).any(axis=1)]\n",
        "    print(f\"Shape after removing NaNs: {embeddings_array.shape}\")\n",
        "\n",
        "    # Check if we have any valid embeddings\n",
        "    if embeddings_array.size > 0:\n",
        "        # Perform K-means clustering\n",
        "        num_clusters = min(3, len(embeddings_array))  # Ensure we don't have more clusters than data points\n",
        "        kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "        cluster_labels = kmeans.fit_predict(embeddings_array)\n",
        "\n",
        "        # Print the clustering results\n",
        "        for i, (doc, label) in enumerate(zip(valid_documents, cluster_labels)):\n",
        "            print(f\"Document {i}: Cluster {label}\")\n",
        "            print(f\"Content: {doc}\")\n",
        "            print()\n",
        "\n",
        "        # Use llm-cluster to generate summaries for each cluster\n",
        "        try:\n",
        "            from llm_cluster import cluster_embeddings\n",
        "            cluster_results = cluster_embeddings(collection, num_clusters, summary=True)\n",
        "\n",
        "            # Print the cluster summaries\n",
        "            for cluster in cluster_results:\n",
        "                print(f\"Cluster {cluster['id']}:\")\n",
        "                print(f\"Summary: {cluster['summary']}\")\n",
        "                print(\"Items:\")\n",
        "                for item in cluster['items']:\n",
        "                    print(f\"- {item['content']}\")\n",
        "                print()\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred while generating cluster summaries: {str(e)}\")\n",
        "    else:\n",
        "        print(\"No valid embeddings after removing NaNs. Cannot perform clustering.\")\n",
        "else:\n",
        "    print(\"No valid embeddings. Cannot perform clustering.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0AomKbO24tc6",
        "outputId": "8d5d9100-36ef-4545-a60c-adec4a81c6c7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Embedding for document 0 is None\n",
            "Warning: Embedding for document 1 is None\n",
            "Warning: Embedding for document 2 is None\n",
            "Warning: Embedding for document 3 is None\n",
            "Warning: Embedding for document 4 is None\n",
            "Warning: Embedding for document 5 is None\n",
            "Warning: Embedding for document 6 is None\n",
            "Warning: Embedding for document 7 is None\n",
            "Warning: Embedding for document 8 is None\n",
            "Warning: Embedding for document 9 is None\n",
            "Number of embeddings: 0\n",
            "No valid embeddings. Cannot perform clustering.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')"
      ],
      "metadata": {
        "id": "cP65e6NI5Wha"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, doc in enumerate(documents):\n",
        "    print(f\"Document {i}: {doc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6R49tUM-8Xe8",
        "outputId": "57aebca0-fe69-48eb-8172-b5fbdfa074d7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document 0: Machine learning is a subset of artificial intelligence.\n",
            "Document 1: Natural language processing deals with the interaction between computers and human language.\n",
            "Document 2: Deep learning uses neural networks with multiple layers.\n",
            "Document 3: Reinforcement learning is learning what to do to maximize a reward.\n",
            "Document 4: Computer vision is the field of AI that trains computers to interpret visual information.\n",
            "Document 5: Clustering is an unsupervised learning technique.\n",
            "Document 6: Classification is a supervised learning task.\n",
            "Document 7: Regression predicts continuous values.\n",
            "Document 8: Neural networks are inspired by the human brain.\n",
            "Document 9: Support vector machines are used for classification and regression tasks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = model.encode(documents)\n",
        "print(f\"Shape of embeddings: {embeddings.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYEGR5fh8Y8w",
        "outputId": "dbd1e2e1-cdd4-4091-933a-cf3f258aff65"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of embeddings: (10, 384)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = []\n",
        "for i, doc in enumerate(documents):\n",
        "    try:\n",
        "        embedding = model.encode(doc)\n",
        "        embeddings.append(embedding)\n",
        "    except Exception as e:\n",
        "        print(f\"Error embedding document {i}: {str(e)}\")\n",
        "\n",
        "embeddings_array = np.array(embeddings)"
      ],
      "metadata": {
        "id": "a8QCP1rL8at9"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 10\n",
        "all_embeddings = []\n",
        "for i in range(0, len(documents), batch_size):\n",
        "    batch = documents[i:i+batch_size]\n",
        "    batch_embeddings = model.encode(batch)\n",
        "    all_embeddings.extend(batch_embeddings)\n",
        "\n",
        "embeddings_array = np.array(all_embeddings)"
      ],
      "metadata": {
        "id": "jW4_vrv78crs"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import csv\n",
        "\n",
        "# Load the sentence transformer model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Read the CSV file and extract the text to be embedded\n",
        "# Create a sample dataset of documents\n",
        "documents = [\n",
        "    \"Machine learning is a subset of artificial intelligence.\",\n",
        "    \"Natural language processing deals with the interaction between computers and human language.\",\n",
        "    \"Deep learning uses neural networks with multiple layers.\",\n",
        "    \"Reinforcement learning is learning what to do to maximize a reward.\",\n",
        "    \"Computer vision is the field of AI that trains computers to interpret visual information.\",\n",
        "    \"Clustering is an unsupervised learning technique.\",\n",
        "    \"Classification is a supervised learning task.\",\n",
        "    \"Regression predicts continuous values.\",\n",
        "    \"Neural networks are inspired by the human brain.\",\n",
        "    \"Support vector machines are used for classification and regression tasks.\"\n",
        "]\n",
        "\n",
        "# Generate embeddings\n",
        "embeddings = model.encode(documents)\n",
        "\n",
        "# Perform K-means clustering\n",
        "num_clusters = 3  # Adjust as needed\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "cluster_labels = kmeans.fit_predict(embeddings)\n",
        "\n",
        "# Print the clustering results\n",
        "for i, (doc, label) in enumerate(zip(documents, cluster_labels)):\n",
        "    print(f\"Document {i}: Cluster {label}\")\n",
        "    print(f\"Content: {doc[:100]}...\")  # Print first 100 characters\n",
        "    print()\n",
        "\n",
        "# Optional: Generate summaries for each cluster\n",
        "# This part depends on how you want to summarize the clusters\n",
        "# You might need to implement a custom summarization method"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5p_LIJ28fi8",
        "outputId": "15b863b2-aba6-4d8c-d0d5-62e84d1981aa"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document 0: Cluster 2\n",
            "Content: Machine learning is a subset of artificial intelligence....\n",
            "\n",
            "Document 1: Cluster 1\n",
            "Content: Natural language processing deals with the interaction between computers and human language....\n",
            "\n",
            "Document 2: Cluster 2\n",
            "Content: Deep learning uses neural networks with multiple layers....\n",
            "\n",
            "Document 3: Cluster 0\n",
            "Content: Reinforcement learning is learning what to do to maximize a reward....\n",
            "\n",
            "Document 4: Cluster 2\n",
            "Content: Computer vision is the field of AI that trains computers to interpret visual information....\n",
            "\n",
            "Document 5: Cluster 1\n",
            "Content: Clustering is an unsupervised learning technique....\n",
            "\n",
            "Document 6: Cluster 1\n",
            "Content: Classification is a supervised learning task....\n",
            "\n",
            "Document 7: Cluster 1\n",
            "Content: Regression predicts continuous values....\n",
            "\n",
            "Document 8: Cluster 2\n",
            "Content: Neural networks are inspired by the human brain....\n",
            "\n",
            "Document 9: Cluster 1\n",
            "Content: Support vector machines are used for classification and regression tasks....\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Document Clustering Analysis\n",
        "\n",
        "This Colab notebook demonstrates the clustering of AI and machine learning-related documents using embeddings and K-means clustering.\n",
        "\n",
        "## Clustering Results\n",
        "\n",
        "The documents were clustered into three groups:\n",
        "\n",
        "### Cluster 0: Reinforcement Learning\n",
        "- Document 3: Reinforcement learning is learning what to do to maximize a reward.\n",
        "\n",
        "### Cluster 1: General ML Techniques and Tasks\n",
        "- Document 1: Natural language processing deals with the interaction between computers and human language.\n",
        "- Document 5: Clustering is an unsupervised learning technique.\n",
        "- Document 6: Classification is a supervised learning task.\n",
        "- Document 7: Regression predicts continuous values.\n",
        "- Document 9: Support vector machines are used for classification and regression tasks.\n",
        "\n",
        "### Cluster 2: Neural Networks and AI Subfields\n",
        "- Document 0: Machine learning is a subset of artificial intelligence.\n",
        "- Document 2: Deep learning uses neural networks with multiple layers.\n",
        "- Document 4: Computer vision is the field of AI that trains computers to interpret visual information.\n",
        "- Document 8: Neural networks are inspired by the human brain.\n",
        "\n",
        "## Analysis of the Clustering\n",
        "\n",
        "1. **Cluster 0 (Reinforcement Learning)**:\n",
        "   - Contains only one document, suggesting that reinforcement learning is semantically distinct from other topics.\n",
        "\n",
        "2. **Cluster 1 (General ML Techniques and Tasks)**:\n",
        "   - Groups various machine learning techniques and tasks.\n",
        "   - Includes both supervised (classification, regression) and unsupervised (clustering) learning methods.\n",
        "   - Natural language processing is included, possibly due to its frequent use of these techniques.\n",
        "\n",
        "3. **Cluster 2 (Neural Networks and AI Subfields)**:\n",
        "   - Focuses on neural networks and broader AI concepts.\n",
        "   - Includes documents mentioning neural networks, deep learning, and computer vision.\n",
        "   - The general concept of machine learning as a subset of AI is also included.\n",
        "\n",
        "## Observations\n",
        "\n",
        "- The clustering algorithm (likely K-means) grouped these documents based on their semantic similarity.\n",
        "- Reinforcement learning was separated into its own cluster, highlighting its distinctiveness.\n",
        "- General machine learning techniques and tasks are grouped together in Cluster 1.\n",
        "- Neural network-related concepts and broader AI subfields are grouped in Cluster 2.\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "This clustering appears reasonable, as it has grouped related concepts together while separating distinct topics. The exact reasons for these groupings depend on the specific embeddings generated for each document and the nature of the clustering algorithm used.\n",
        "\n"
      ],
      "metadata": {
        "id": "WPFYhY7x9DVT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zRrp6w1i8p4V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}